{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis and Baseline modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The aim is to evaluate a **RandomForestRegressor** like a baseline model to estimate PM10.\n",
    "The following experiments are executed:\n",
    "  * **Experiment 1:** Use all available data to train and evaluate a model\n",
    "  * **Experiment 2:** Cross validation between stations\n",
    "  * **Experiment 3:** Compare models by sensor frequency\n",
    "  * **Experiment 4:** Compare models by satelite and frequency\n",
    "\n",
    "Also, other models are evaluated to compare against **RandomForestRegressor**:\n",
    "  * **KNRegressor**\n",
    "  * **MLPRegressor**\n",
    "  * **Linear SVR**\n",
    "  * **XGBRegressor**\n",
    "  \n",
    "  \n",
    "**Hypothesis**\n",
    "  1. RandomForestRegressor is a good baseline model\n",
    "  2. There is another model tath beats RandomForestRegressor\n",
    "  \n",
    "**Notes:**\n",
    "\n",
    "The dataset can be downloaded [here](https://drive.google.com/file/d/1W3JmH32LGJti1jg6Zhq-DQZds21NTREM/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib import rcParams\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from typing import Any, Callable, Dict, Iterable, List\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Tabla_Entrenamiento_MAIAC_MERRA_VIIRS_DEM_PM.csv\"\n",
    "TARGET_COL = \"PM10_valor\"\n",
    "FEATURES_COLS = [\n",
    "    'valor_AOD', 'DEM_asnm', 'VIIRS_night_lights', 'ALBEDO', 'BCCMASS', 'CLDHGH', \n",
    "    'CLDLOW', 'DMSSMASS', 'DUSMASS', 'OCSMASS', 'PBLH', 'PRECTOT','PS', 'RH', \n",
    "    'SO2SMASS', 'SO4SMASS', 'SPEED', 'SPEEDMAX','SSSMASS', 'T', 'U', 'USTAR', 'V'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df: pd.DataFrame, n_sample: int=5) -> None:\n",
    "    \"\"\"\n",
    "    Show brief summary for a given Dataframe\n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        n_sample: number of sample to display\n",
    "    \"\"\"\n",
    "    rows, columns = df.shape\n",
    "    display(HTML(f'<b>Nº Rows:</b> {rows}'))\n",
    "    display(HTML(f'<b>Nº Columns:</b> {columns}'))\n",
    "    display(HTML(f'<b>Sample:</b>'))\n",
    "    display(df.sample(n_sample))\n",
    "    \n",
    "\n",
    "def get_metrics(forecast: np.array, ground_truth: np.array) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate a set of metrics to measure the performance of the forecasting model\n",
    "\n",
    "    Args:\n",
    "        forecast: list of forecasted values\n",
    "        ground_truth: list of real values\n",
    "\n",
    "    Return:\n",
    "        Dictionary of the main forcasting metrics\n",
    "    \"\"\"\n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs(forecast - ground_truth) / ground_truth)\n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(forecast, ground_truth)\n",
    "    # Root Mean Square Error\n",
    "    rmse = np.sqrt(mean_squared_error(forecast, ground_truth))\n",
    "    # Mean Shortage Error\n",
    "    mshe = np.nanmean(\n",
    "        np.where(forecast < ground_truth, np.abs(forecast - ground_truth), np.nan)\n",
    "    )\n",
    "    # Mean Surplus Error\n",
    "    msue = np.nanmean(\n",
    "        np.where(forecast > ground_truth, np.abs(forecast - ground_truth), np.nan)\n",
    "    )\n",
    "    \n",
    "    accuracy = None\n",
    "    if mape <= 1:\n",
    "        accuracy = (1 - mape) * 100\n",
    "        \n",
    "    return {\n",
    "        \"mape\": mape,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"mae\": mae,\n",
    "        \"mshe\": mshe,\n",
    "        \"msue\": msue,\n",
    "        \"rmse\": rmse,\n",
    "        \"n_sample\": len(ground_truth),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def search_best_model(X: np.array, y:np.array, model: Any, param_grid: Dict) -> Any:\n",
    "    \"\"\"\n",
    "    Randomized search on hyperparameters to get the best model\n",
    "    \n",
    "    Args:\n",
    "        X: dataset to train\n",
    "        y: targets to train\n",
    "        model: model instance to fit\n",
    "        param_grid: set of parameter to try \n",
    "    Return:\n",
    "        The best model fitted\n",
    "    \"\"\"\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rs = RandomizedSearchCV(\n",
    "                estimator=model,\n",
    "                param_distributions=param_grid,\n",
    "                n_iter = 100,\n",
    "                cv = 3,\n",
    "                verbose=2,\n",
    "                random_state=42,\n",
    "                n_jobs = -1\n",
    "            )\n",
    "    \n",
    "    rs.fit(X, y)\n",
    "    \n",
    "    return rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NA values from target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[TARGET_COL], inplace=True)\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NA values from feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', inplace=True)\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[FEATURES_COLS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target (PM10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[TARGET_COL].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df, x=TARGET_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PM10 distribution by signal frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"AODnm\")[TARGET_COL].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PM10 distribution by sensor and signal frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"AODnm\", \"Satelite\"])[TARGET_COL].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PM10 distribution by monitoring station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"estacion_pm\"])[TARGET_COL].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1:  Use all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[FEATURES_COLS], df[TARGET_COL], test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set param grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "rf_param_grid =  {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best params\n",
    "display(HTML(f'<b>Randomized Search CV:</b>'))\n",
    "rf =  RandomForestRegressor()\n",
    "rf_random = search_best_model(X_train, y_train, rf, rf_param_grid)\n",
    "\n",
    "display(HTML(f'<b>Best params:</b>'))\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training RF estimator with the best parameters to get feature importances\n",
    "rf.set_params(**rf_random.best_params_)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "print(get_metrics(np.array(y_train), rf.predict(X_train)))\n",
    "\n",
    "# Test metrics\n",
    "display(HTML(f'<b>Evaluation metrics:</b>'))\n",
    "print(get_metrics(np.array(y_test), rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=y_test, y=rf.predict(X_test))\n",
    "ax = ax.set(xlabel = \"PM10\", ylabel=\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importances = list(zip(FEATURES_COLS, np.round(rf.feature_importances_, 2)))\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "_ = [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Cross validation between stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_metrics = {'train':{}, 'test':{}}\n",
    "for station in pd.unique(df.estacion_pm):\n",
    "    display(HTML(f'<b>Station to evaluate:</b> {station}'))\n",
    "    mask = df.estacion_pm == station\n",
    "    X_train = df[~mask][FEATURES_COLS]\n",
    "    y_train = df[~mask][TARGET_COL]\n",
    "    X_test = df[mask][FEATURES_COLS]\n",
    "    y_test = df[mask][TARGET_COL]\n",
    "    \n",
    "    \n",
    "    # Get best params\n",
    "    display(HTML(f'<b>Randomized Search CV:</b>'))\n",
    "    rf = search_best_model(X_train, y_train, RandomForestRegressor(), rf_param_grid)\n",
    "    \n",
    "    display(HTML(f'<b>Best params:</b>'))\n",
    "    print(rf.best_params_)\n",
    "    \n",
    "    # Training metrics\n",
    "    station_metrics['train'][station] = get_metrics(np.array(y_train), rf.predict(X_train))\n",
    "\n",
    "    # Test metrics\n",
    "    station_metrics['test'][station] = get_metrics(np.array(y_test), rf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "display(pd.DataFrame(station_metrics['train']))\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "display(pd.DataFrame(station_metrics['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Compare models by sensor frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_metrics = {'train':{}, 'test':{}}\n",
    "for freq in pd.unique(df.AODnm):\n",
    "    display(HTML(f'<b>Frequency to evaluate:</b> {freq}'))\n",
    "    freq_df = df[df.AODnm == freq]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                        freq_df[FEATURES_COLS],\n",
    "                                                        freq_df[TARGET_COL],\n",
    "                                                        test_size = 0.20,\n",
    "                                                        random_state = 42\n",
    "                                                    )\n",
    "    \n",
    "    # Get best params\n",
    "    display(HTML(f'<b>Randomized Search CV:</b>'))\n",
    "    rf = search_best_model(X_train, y_train, RandomForestRegressor(), rf_param_grid)\n",
    "    \n",
    "    display(HTML(f'<b>Best params:</b>'))\n",
    "    print(rf.best_params_)\n",
    "    \n",
    "    # Training metrics\n",
    "    freq_metrics['train'][freq] = get_metrics(np.array(y_train), rf.predict(X_train))\n",
    "\n",
    "    # Test metrics\n",
    "    freq_metrics['test'][freq] = get_metrics(np.array(y_test), rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "display(pd.DataFrame(freq_metrics['train']))\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "display(pd.DataFrame(freq_metrics['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Compare models by satelite and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_freq_metrics = {'train':{}, 'test':{}}\n",
    "for name, group  in df.groupby([\"AODnm\", \"Satelite\"]):\n",
    "    freq, sat = name\n",
    "    display(HTML(f'<b>Frequency to evaluate:</b> {freq}'))\n",
    "    display(HTML(f'<b>Satelite to evaluate:</b> {sat}'))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                        group[FEATURES_COLS],\n",
    "                                                        group[TARGET_COL],\n",
    "                                                        test_size = 0.20,\n",
    "                                                        random_state = 42\n",
    "                                                    )\n",
    "    \n",
    "    # Get best params\n",
    "    display(HTML(f'<b>Randomized Search CV:</b>'))\n",
    "    rf = search_best_model(X_train, y_train, RandomForestRegressor(), rf_param_grid)\n",
    "    \n",
    "    display(HTML(f'<b>Best params:</b>'))\n",
    "    print(rf.best_params_)\n",
    "    \n",
    "    \n",
    "    # Training metrics\n",
    "    sat_freq_metrics['train'][f'{sat}_{freq}'] = get_metrics(np.array(y_train), rf.predict(X_train))\n",
    "\n",
    "    # Test metrics\n",
    "    sat_freq_metrics['test'][f'{sat}_{freq}'] = get_metrics(np.array(y_test), rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "display(pd.DataFrame(sat_freq_metrics['train']))\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "display(pd.DataFrame(sat_freq_metrics['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models using all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[FEATURES_COLS], df[TARGET_COL], test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "kn_param_grid = {\n",
    "    'n_neighbors': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [30, 40, 50],\n",
    "    'p': [1,2]\n",
    "    \n",
    "}\n",
    "\n",
    "# Get best params\n",
    "display(HTML(f'<b>Randomized Search CV:</b>'))\n",
    "kn = search_best_model(X_train, y_train, KNeighborsRegressor(), knn_param_grid)\n",
    "\n",
    "display(HTML(f'<b>Best params:</b>'))\n",
    "print(knn.best_params_)\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "print(get_metrics(np.array(y_train), kn.predict(X_train)))\n",
    "\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "print(get_metrics(np.array(y_test), kn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svr_param_grid = {\n",
    "    'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'] ,\n",
    "    'fit_intercept': [True, False],\n",
    "    'dual': [True, False],\n",
    "}\n",
    "\n",
    "\n",
    "svr = search_best_model(X_train, y_train, LinearSVR(), svr_param_grid)\n",
    "\n",
    "display(HTML(f'<b>Best params:</b>'))\n",
    "print(svr.best_params_)\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "print(get_metrics(np.array(y_train), svr.predict(X_train)))\n",
    "\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "print(get_metrics(np.array(y_test), svr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(8, 8, 64), (16, 16, 128), (32, 32, 256), (64, 64, 512)] ,\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'adam'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'max_iter': [1000],\n",
    "}\n",
    "\n",
    "\n",
    "nn = search_best_model(X_train, y_train, MLPRegressor(), mlp_param_grid)\n",
    "\n",
    "display(HTML(f'<b>Best params:</b>'))\n",
    "print(nn.best_params_)\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "print(get_metrics(np.array(y_train), nn.predict(X_train)))\n",
    "\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "print(get_metrics(np.array(y_test), nn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [5, 10, 50, 100, 300, 1000] ,\n",
    "    'max_depth': [5, 10, 20, 50, 70, 100],\n",
    "    'objective': ['reg:linear'],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart']\n",
    "}\n",
    "\n",
    "\n",
    "xgb = search_best_model(X_train, y_train, XGBRegressor(), xgb_param_grid)\n",
    "\n",
    "display(HTML(f'<b>Best params:</b>'))\n",
    "print(xgb.best_params_)\n",
    "\n",
    "\n",
    "# Get metrics\n",
    "display(HTML(f'<b>Training metrics:</b>'))\n",
    "print(get_metrics(np.array(y_train), xgb.predict(X_train)))\n",
    "\n",
    "display(HTML(f'<b>Test metrics:</b>'))\n",
    "print(get_metrics(np.array(y_test), xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=y_test, y=xgb.predict(X_test))\n",
    "ax = ax.set(xlabel = \"PM10\", ylabel=\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* **RandomForestRegressor** is a good baseline model to estimate PM10\n",
    "* **XGBRegressor** is the best approach outperforming Random Forest Regressor by ~ 1 percentage point, 94.84% and 93.62% accuracy respectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
